{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndefine set of models to try run\\ndefine the corresponding hyperparameter\\ntune the hyperparameters\\nretrieve the best model\\ntrain the best model\\ntest the best model'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "define set of models to try run\n",
    "define the corresponding hyperparameter\n",
    "tune the hyperparameters\n",
    "retrieve the best model\n",
    "train the best model\n",
    "test the best model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.mlproject.utils.common import read_yaml_keys,read_yaml,create_directories\n",
    "from src.mlproject.constants import CONFIG_PATH_YAML,SCHEMA_PATH_YAML,PARAMS_PATH_YAML\n",
    "from src.mlproject.entity.config_entity import DataTransformationConfig\n",
    "import os\n",
    "#hyperparameter tuning.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "   train_csv:Path# what we need is train csv, where was it stored\n",
    "   test_csv:Path\n",
    "   model_file: Path#what we need is model file,where will we store it?\n",
    "   schema_data:Path\n",
    "   report_file: Path#we will also need a report of all the models performcance,where will we store it?\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurationManager:\n",
    "\n",
    "   def __init__(self,config_filepath=CONFIG_PATH_YAML,schema_path=SCHEMA_PATH_YAML,params_path=PARAMS_PATH_YAML) :#inner folder,pathto csv,datastore\n",
    "        self.config_data=read_yaml(config_filepath)\n",
    "        self.params=read_yaml(params_path)\n",
    "        print(\"Loaded params:\", self.params)\n",
    "        self.schema=read_yaml(schema_path)\n",
    "        #Retrieve the parent folder \n",
    "        self.parent_folder =self.config_data.get('parent_folder', '')\n",
    "         #Retrieve the artifcats root \n",
    "        self.artifacts_root =self.parent_folder.get('artifacts_root', '')\n",
    "        #and create the folder 'artifacts_root'\n",
    "        create_directories([self.artifacts_root])\n",
    "\n",
    "         #retrieve the model trainer\n",
    "        self.model_trainer=self.config_data.get('model_trainer','')\n",
    "         #get the model trainer inner folder\n",
    "        \n",
    "        create_directories([self.model_trainer.get('model_file_innerfolder','')])\n",
    "        #join the file\n",
    "        self.model_file_path=os.path.join(self.model_trainer.get('model_file_innerfolder',''),self.model_trainer.get('model_file_path',''))\n",
    "        self.report_file_path=os.path.join(self.model_trainer.get('model_file_innerfolder',''),self.model_trainer.get('report_file_path',''))\n",
    "   \n",
    "   \n",
    "   def get_model_trainer_config(self)-> ModelTrainerConfig:\n",
    "       get_model_trainer=ModelTrainerConfig(\n",
    "           train_csv=self.model_trainer.get('train_csv',''), #this is where it was stores\n",
    "           test_csv=self.model_trainer.get('test_csv',''),#i was using self.config.get instead of model trainer,so geting wrror cannot eopen empty string'''\n",
    "           model_file=self.model_file_path,#this is where it will be stored\n",
    "           schema_data=self.schema,\n",
    "           report_file=self.report_file_path\n",
    "           \n",
    "       )\n",
    "\n",
    "       return get_model_trainer\n",
    "\n",
    "    \n",
    "      \n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEntity:\n",
    "    models={'RandomForest':RandomForestRegressor(),\n",
    "                    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "                    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "                    \"LinearRegression\": LinearRegression(),\n",
    "                   \n",
    "                    #\"CatBoosting Regressor\": CatBoostRegressor(verbose=False),#\n",
    "                    \"AdaBoost Regressor\": AdaBoostRegressor()\n",
    "                    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.mlproject.utils.common import evaluate_score,save_model\n",
    "import mlflow\n",
    "from mlflow import sklearn\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "class ModelTrainerService:\n",
    "    def __init__(self,config_instance:ModelTrainerConfig,models_entity:ModelEntity) :\n",
    "        self.config_instance=config_instance\n",
    "        self.model_train=config_instance.train_csv\n",
    "        self.model_test=config_instance.test_csv\n",
    "        self.model_pickle=config_instance.model_file\n",
    "        self.report_file=config_instance.report_file\n",
    "        \n",
    "        self.model_schema=self.config_instance.schema_data\n",
    "        \n",
    "        #self.model_schema=self.config_instance.schema_data,# why is it that if we put a comma it becomes a tuple\n",
    "        \n",
    "        self.models_dict=models_entity.models\n",
    "\n",
    "        configurationmanagerinstance=configurationManager()\n",
    "        self.parameters=configurationmanagerinstance.params\n",
    "        self.hyperparameter=self.parameters.get('hyperparameters',{})\n",
    "        print(\"Loaded hyperparam:\", self.hyperparameter)\n",
    "        \n",
    "    def splitting(self):\n",
    "        cat_col,num_cal,target_col=read_yaml_keys(self.model_schema)\n",
    "        train_df=pd.read_csv(self.config_instance.train_csv,header='infer',delimiter=',')\n",
    "        #train_df=pd.read_csv(filepath_or_buffer=self.model_train)\n",
    "        test_df=pd.read_csv(self.model_test)\n",
    "\n",
    "        x_train=train_df.drop(target_col,axis=1)\n",
    "        y_train=train_df[target_col]\n",
    "\n",
    "        x_test=test_df.drop(target_col,axis=1)\n",
    "        y_test=test_df[target_col]\n",
    "\n",
    "        return x_test,y_test,x_train,y_train\n",
    "    \n",
    "    \n",
    "    def get_best_model(self):\n",
    "        report={}\n",
    "        x_test,y_test,x_train,y_train=self.splitting()\n",
    "        '''for i in range(len(self.models_dict)):\n",
    "           print('..........................................')\n",
    "           model_name =list(self.models_dict.keys())[i]\n",
    "           print(model_name)\n",
    "           model_instance =list(self.models_dict.values())[i]\n",
    "           \n",
    "           parameters=self.hyperparameter.get(list(self.models_dict.keys())[i],{})\n",
    "           print(parameters)'''\n",
    "\n",
    "           \n",
    "        for model_name,model_instance in self.models_dict.items():\n",
    "            print('..........................................')\n",
    "           # Iterate over models and hyperparameters.\n",
    "            parameters=self.hyperparameter.get(model_name,{})# i forgot to use get to retrieve the model name such that is resulted into category name and attributes but i wanted attributes only,the \n",
    "            \n",
    "            if parameters:#check if parameters is not empty\n",
    "                     rs=RandomizedSearchCV(estimator=model_instance,param_distributions=parameters,cv=5)\n",
    "                     rs.fit(x_train,y_train)\n",
    "                     predict=rs.predict(x_test)\n",
    "                     mae,mse,score=evaluate_score(y_test,predicted_value=predict)\n",
    "                     #tuning the parameters manually below has resulted in error in passing the hyperparameters,using Gridsearch cv takes forever\n",
    "                     '''for key, value in parameters.items():\n",
    "                     model_instance.set_params(**{key: value})\n",
    "                     #model_instance.set_params(**parameters)\n",
    "                     model_instance.fit(x_train,y_train)\n",
    "                     y_pred=model_instance.predict(x_test)\n",
    "                     score=r2_score(y_test,y_pred)'''\n",
    "                #write to the report file created in the self.report attribute by opening it in append mode and saving the results stored in the variable\n",
    "                     try:\n",
    "                        with open(self.report_file, 'a') as file:\n",
    "                            file.write(f\"{model_name}: {score} \\t {mae}\\t {mse} \\n\")\n",
    "                        print(f\"Score for {model_name} written to report file.\")\n",
    "                     except Exception as e:\n",
    "                        print(f\"Error writing to report file: {e}\")\n",
    "\n",
    "                     try:\n",
    "                        with mlflow.start_run(run_name='tired_3') as run:\n",
    "                                mlflow.log_metric('r2_score', score)\n",
    "                                mlflow.log_metric('mae', mae)\n",
    "                                mlflow.log_metric('mse', mse)\n",
    "                                mlflow.log_params(rs.best_params_)\n",
    "                                mlflow.sklearn.log_model(rs.best_estimator_, \"model_\" + model_name)\n",
    "                                # Print the run id of each model but its not a must\n",
    "                                print(f\"Run ID: {run.info.run_id}\")\n",
    "                               # runid=run.info.run_id\n",
    "                        print(f\"Model {model_name} logged to mlflow.\")\n",
    "                     except Exception as e:\n",
    "                            print(f\"Error logging to mlflow: {e}\")\n",
    "\n",
    "        best_run_id='f95b9d08d6404162815bb3c19fce79f1'\n",
    "        \n",
    "        best_run=mlflow.get_run(best_run_id)#i was using search_runs so it resulted into empty runs.\n",
    "        if  best_run:\n",
    "            best_hyperparameters= best_run.data.params#used to get the hyperparameters\n",
    "            best_model=AdaBoostRegressor(**best_hyperparameters)#assigns the hyperparameters to the model\n",
    "            save_model(model=best_model,path=self.model_pickle)\n",
    "        \n",
    "        else:\n",
    "             print(f\"No runs found for run_id: {best_run_id}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "                    \n",
    "                \n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "           \n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "           \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded params: {'hyperparameters': {'DecisionTree': {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'splitter': ['best', 'random'], 'max_features': ['sqrt', 'log2']}, 'GradientBoosting': {'loss': ['squared_error', 'huber', 'absolute_error', 'quantile'], 'learning_rate': [0.1, 0.01, 0.05, 0.001], 'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9], 'criterion': ['squared_error', 'friedman_mse'], 'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [8, 16, 32, 64, 128, 256]}, 'Linear Regression': {}, 'XGBRegressor': {'learning_rate': [0.1, 0.01, 0.05, 0.001], 'n_estimators': [8, 16, 32, 64, 128, 256]}, 'AdaBoost Regressor': {'learning_rate': [0.1, 0.01, 0.5, 0.001], 'loss': ['linear', 'square', 'exponential'], 'n_estimators': [8, 16, 32, 64, 128, 256]}}}\n",
      "Loaded params: {'hyperparameters': {'DecisionTree': {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'splitter': ['best', 'random'], 'max_features': ['sqrt', 'log2']}, 'GradientBoosting': {'loss': ['squared_error', 'huber', 'absolute_error', 'quantile'], 'learning_rate': [0.1, 0.01, 0.05, 0.001], 'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9], 'criterion': ['squared_error', 'friedman_mse'], 'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [8, 16, 32, 64, 128, 256]}, 'Linear Regression': {}, 'XGBRegressor': {'learning_rate': [0.1, 0.01, 0.05, 0.001], 'n_estimators': [8, 16, 32, 64, 128, 256]}, 'AdaBoost Regressor': {'learning_rate': [0.1, 0.01, 0.5, 0.001], 'loss': ['linear', 'square', 'exponential'], 'n_estimators': [8, 16, 32, 64, 128, 256]}}}\n",
      "Loaded hyperparam: {'DecisionTree': {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'splitter': ['best', 'random'], 'max_features': ['sqrt', 'log2']}, 'GradientBoosting': {'loss': ['squared_error', 'huber', 'absolute_error', 'quantile'], 'learning_rate': [0.1, 0.01, 0.05, 0.001], 'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9], 'criterion': ['squared_error', 'friedman_mse'], 'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [8, 16, 32, 64, 128, 256]}, 'Linear Regression': {}, 'XGBRegressor': {'learning_rate': [0.1, 0.01, 0.05, 0.001], 'n_estimators': [8, 16, 32, 64, 128, 256]}, 'AdaBoost Regressor': {'learning_rate': [0.1, 0.01, 0.5, 0.001], 'loss': ['linear', 'square', 'exponential'], 'n_estimators': [8, 16, 32, 64, 128, 256]}}\n",
      "..........................................\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for DecisionTree written to report file.\n",
      "Run ID: f1ad8170127d4cecb48166434a2fad48\n",
      "Model DecisionTree logged to mlflow.\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\studenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\studenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\studenv\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\studenv\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\studenv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\User\\anaconda3\\envs\\studenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [ 0.47789002  0.69129314  0.48449333 -1.81338795         nan  0.00392963\n",
      " -1.63228446         nan         nan  0.84968359]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for GradientBoosting written to report file.\n",
      "Run ID: 3f9adf4d8bb84ebcb4b1d34a45cb9f22\n",
      "Model GradientBoosting logged to mlflow.\n",
      "..........................................\n",
      "..........................................\n",
      "Score for AdaBoost Regressor written to report file.\n",
      "Run ID: 6f18551d8513475282bbfa2e45c1d63c\n",
      "Model AdaBoost Regressor logged to mlflow.\n",
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "class ModelTrainerPipeline:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def main(self):\n",
    "        config_manager=configurationManager()\n",
    "        model_config=config_manager.get_model_trainer_config()\n",
    "        model_entity=ModelEntity()\n",
    "        \n",
    "        model_service=ModelTrainerService(model_config,model_entity)\n",
    "        model_service.get_best_model()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ModelTrainerPipeline().main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
